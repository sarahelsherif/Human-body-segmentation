{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import numpy as np\n",
    "import flax\n",
    "from flax.core import FrozenDict, freeze\n",
    "from flax.traverse_util import unflatten_dict\n",
    "from flax import linen as nn\n",
    "from flax.linen import BatchNorm, Conv\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchvision==0.11.1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from typing import (Any, Callable, Dict, Iterable, Mapping, Optional, Sequence, Tuple, Union)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet model structure in JAX\n",
    "STAGE_SIZES = {\n",
    "    18: [2, 2, 2, 2],\n",
    "    34: [3, 4, 6, 3],\n",
    "    50: [3, 4, 6, 3],\n",
    "    101: [3, 4, 23, 3],\n",
    "    152: [3, 8, 36, 3],\n",
    "    200: [3, 24, 36, 3],\n",
    "    269: [3, 30, 48, 8],\n",
    "}\n",
    "ModuleDef = Callable[..., Callable]\n",
    "# InitFn = Callable[[PRNGKey, Shape, DType], Array]\n",
    "InitFn = Callable[[Any, Iterable[int], Any], Any]\n",
    "#conv \n",
    " \n",
    "class ConvBlock(nn.Module): \n",
    "        n_filters: int\n",
    "        kernel_size: Tuple[int, int] = (3, 3)\n",
    "        strides: Tuple[int, int] = (1, 1)\n",
    "        activation: Callable = nn.relu\n",
    "        padding: Union[str, Iterable[Tuple[int, int]]] = ((0, 0), (0, 0))\n",
    "        is_last: bool = False\n",
    "        groups: int = 1\n",
    "        kernel_init: InitFn = nn.initializers.kaiming_normal()\n",
    "        bias_init: InitFn = nn.initializers.zeros\n",
    "\n",
    "        conv_cls: ModuleDef = nn.Conv\n",
    "        norm_cls: Optional[ModuleDef] = partial(nn.BatchNorm, momentum=0.9)\n",
    "\n",
    "        force_conv_bias: bool = False\n",
    "\n",
    "        @nn.compact\n",
    "        def __call__(self, x):\n",
    "            x = self.conv_cls(\n",
    "                self.n_filters,\n",
    "                self.kernel_size,\n",
    "                self.strides,\n",
    "                use_bias=(not self.norm_cls or self.force_conv_bias),\n",
    "                padding=self.padding,\n",
    "                feature_group_count=self.groups,\n",
    "                kernel_init=self.kernel_init,\n",
    "                bias_init=self.bias_init,\n",
    "            )(x)\n",
    "            if self.norm_cls:\n",
    "                scale_init = (nn.initializers.zeros\n",
    "                              if self.is_last else nn.initializers.ones)\n",
    "                mutable = self.is_mutable_collection('batch_stats')\n",
    "                x = self.norm_cls(use_running_average=not mutable, scale_init=scale_init)(x)\n",
    "\n",
    "            if not self.is_last:\n",
    "                x = self.activation(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "class Sequential(nn.Module):\n",
    "    layers: Sequence[Union[nn.Module, Callable[[jnp.ndarray], jnp.ndarray]]]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "def slice_variables(variables: Mapping[str, Any],\n",
    "                    start: int = 0,\n",
    "                    end: Optional[int] = None) -> flax.core.FrozenDict:\n",
    "    \"\"\"Returns variables dict correspond to a sliced model.\n",
    "    Args:\n",
    "        variables: A dict (typically a flax.core.FrozenDict) containing the\n",
    "            model parameters and state.\n",
    "        start: integer indicating the first layer to keep.\n",
    "        end: integer indicating the first layer to exclude (can be negative,\n",
    "            has the same semantics as negative list indexing).\n",
    "    Returns:\n",
    "        A flax.core.FrozenDict with the subset of parameters/state requested.\n",
    "    \"\"\"\n",
    "    last_ind = max(int(s.split('_')[-1]) for s in variables['params'])\n",
    "    if end is None:\n",
    "        end = last_ind + 1\n",
    "    elif end < 0:\n",
    "        end += last_ind + 1\n",
    "\n",
    "    sliced_variables: Dict[str, Any] = {}\n",
    "    for k, var_dict in variables.items():  # usually params and batch_stats\n",
    "        sliced_variables[k] = {\n",
    "            f'layers_{i-start}': var_dict[f'layers_{i}']\n",
    "            for i in range(start, end)\n",
    "            if f'layers_{i}' in var_dict\n",
    "        }\n",
    "\n",
    "    return flax.core.freeze(sliced_variables)    \n",
    "#---------\n",
    "#softmax func\n",
    "def rsoftmax(x, radix, cardinality):\n",
    "    # (batch_size, features) -> (batch_size, features)\n",
    "    batch = x.shape[0]\n",
    "    if radix > 1:\n",
    "        x = x.reshape((batch, cardinality, radix, -1)).swapaxes(1, 2)\n",
    "        return nn.softmax(x, axis=1).reshape((batch, -1))\n",
    "    else:\n",
    "        return nn.sigmoid(x)\n",
    "\n",
    "#conv2d\n",
    "class SplAtConv2d(nn.Module):\n",
    "    channels: int\n",
    "    kernel_size: Tuple[int, int]\n",
    "    strides: Tuple[int, int] = (1, 1)\n",
    "    padding: Union[str, Iterable[Tuple[int, int]]] = ((0, 0), (0, 0))\n",
    "    groups: int = 1\n",
    "    radix: int = 2\n",
    "    reduction_factor: int = 4\n",
    "    conv_block_cls: ModuleDef = ConvBlock\n",
    "    cardinality: int = groups\n",
    "    match_reference: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        inter_channels = max(x.shape[-1] * self.radix // self.reduction_factor, 32)\n",
    "\n",
    "        conv_block = self.conv_block_cls(self.channels * self.radix,\n",
    "                                         kernel_size=self.kernel_size,\n",
    "                                         strides=self.strides,\n",
    "                                         groups=self.groups * self.radix,\n",
    "                                         padding=self.padding)\n",
    "        conv_cls = conv_block.conv_cls  # type: ignore\n",
    "        x = conv_block(x)\n",
    "\n",
    "        if self.radix > 1:\n",
    "            # torch split takes split_size: int(rchannel//self.radix)\n",
    "            # jnp split takes num sections: self.radix\n",
    "            split = jnp.split(x, self.radix, axis=-1)\n",
    "            gap = sum(split)\n",
    "        else:\n",
    "            gap = x\n",
    "\n",
    "        gap = gap.mean((1, 2), keepdims=True)  # type: ignore # global average pool\n",
    "\n",
    "        # Remove force_conv_bias after resolving\n",
    "        # github.com/zhanghang1989/ResNeSt/issues/125\n",
    "        gap = self.conv_block_cls(inter_channels,\n",
    "                                  kernel_size=(1, 1),\n",
    "                                  groups=self.cardinality,\n",
    "                                  force_conv_bias=self.match_reference)(gap)\n",
    "\n",
    "        attn = conv_cls(self.channels * self.radix,\n",
    "                        kernel_size=(1, 1),\n",
    "                        feature_group_count=self.cardinality)(gap)  # n x 1 x 1 x c\n",
    "        attn = attn.reshape((x.shape[0], -1))\n",
    "        attn = rsoftmax(attn, self.radix, self.cardinality)\n",
    "        attn = attn.reshape((x.shape[0], 1, 1, -1))\n",
    "\n",
    "        if self.radix > 1:\n",
    "            attns = jnp.split(attn, self.radix, axis=-1)\n",
    "            out = sum(a * s for a, s in zip(attns, split))\n",
    "        else:\n",
    "            out = attn * x\n",
    "\n",
    "        return out\n",
    "#------------------------------------------------------------------\n",
    "class ResNetStem(nn.Module):\n",
    "    conv_block_cls: ModuleDef = ConvBlock\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        return self.conv_block_cls(64,\n",
    "                                   kernel_size=(7, 7),\n",
    "                                   strides=(2, 2),\n",
    "                                   padding=[(3, 3), (3, 3)])(x) #################\n",
    "\n",
    "class ResNetDStem(nn.Module):\n",
    "    conv_block_cls: ModuleDef = ConvBlock\n",
    "    stem_width: int = 32\n",
    "\n",
    "    # If True, n_filters for first conv is (input_channels + 1) * 8\n",
    "    adaptive_first_width: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        cls = partial(self.conv_block_cls, kernel_size=(3, 3), padding=((1, 1), (1, 1)))\n",
    "        first_width = (8 * (x.shape[-1] + 1)\n",
    "                       if self.adaptive_first_width else self.stem_width)\n",
    "        x = cls(first_width, strides=(2, 2))(x)\n",
    "        x = cls(self.stem_width, strides=(1, 1))(x)\n",
    "        x = cls(self.stem_width * 2, strides=(1, 1))(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNetSkipConnection(nn.Module):\n",
    "    strides: Tuple[int, int]\n",
    "    conv_block_cls: ModuleDef = ConvBlock\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, out_shape):\n",
    "        if x.shape != out_shape:\n",
    "            x = self.conv_block_cls(out_shape[-1],\n",
    "                                    kernel_size=(1, 1),\n",
    "                                    strides=self.strides,\n",
    "                                    activation=lambda y: y)(x)\n",
    "        return x\n",
    "\n",
    "class ResNetDSkipConnection(nn.Module):\n",
    "    strides: Tuple[int, int]\n",
    "    conv_block_cls: ModuleDef = ConvBlock\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, out_shape):\n",
    "        if self.strides != (1, 1):\n",
    "            x = nn.avg_pool(x, (2, 2), strides=(2, 2), padding=((0, 0), (0, 0)))\n",
    "        if x.shape[-1] != out_shape[-1]:\n",
    "            x = self.conv_block_cls(out_shape[-1], (1, 1), activation=lambda y: y)(x)\n",
    "        return x\n",
    "\n",
    "class ResNeStSkipConnection(ResNetDSkipConnection):\n",
    "    # Inheritance to ensures our variables dict has the right names.\n",
    "    pass\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    n_hidden: int\n",
    "    strides: Tuple[int, int] = (1, 1)\n",
    "\n",
    "    activation: Callable = nn.relu\n",
    "    conv_block_cls: ModuleDef = ConvBlock\n",
    "    skip_cls: ModuleDef = ResNetSkipConnection\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        skip_cls = partial(self.skip_cls, conv_block_cls=self.conv_block_cls)\n",
    "        y = self.conv_block_cls(self.n_hidden,\n",
    "                                padding=[(1, 1), (1, 1)],\n",
    "                                strides=self.strides)(x)\n",
    "        y = self.conv_block_cls(self.n_hidden, padding=[(1, 1), (1, 1)],\n",
    "                                is_last=True)(y)\n",
    "        return self.activation(y + skip_cls(self.strides)(x, y.shape))\n",
    "\n",
    "class ResNetBottleneckBlock(nn.Module):\n",
    "    n_hidden: int\n",
    "    strides: Tuple[int, int] = (1, 1)\n",
    "    expansion: int = 4\n",
    "    groups: int = 1  # cardinality\n",
    "    base_width: int = 64\n",
    "\n",
    "    activation: Callable = nn.relu\n",
    "    conv_block_cls: ModuleDef = ConvBlock\n",
    "    skip_cls: ModuleDef = ResNetSkipConnection\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        skip_cls = partial(self.skip_cls, conv_block_cls=self.conv_block_cls)\n",
    "        group_width = int(self.n_hidden * (self.base_width / 64.)) * self.groups\n",
    "\n",
    "        # Downsampling strides in 3x3 conv instead of 1x1 conv, which improves accuracy.\n",
    "        # This variant is called ResNet V1.5 (matches torchvision).\n",
    "        y = self.conv_block_cls(group_width, kernel_size=(1, 1))(x)\n",
    "        y = self.conv_block_cls(group_width,\n",
    "                                strides=self.strides,\n",
    "                                groups=self.groups,\n",
    "                                padding=((1, 1), (1, 1)))(y)\n",
    "        y = self.conv_block_cls(self.n_hidden * self.expansion,\n",
    "                                kernel_size=(1, 1),\n",
    "                                is_last=True)(y)\n",
    "        return self.activation(y + skip_cls(self.strides)(x, y.shape))\n",
    "\n",
    "\n",
    "class ResNetDBlock(ResNetBlock):\n",
    "    skip_cls: ModuleDef = ResNetDSkipConnection\n",
    "\n",
    "\n",
    "class ResNetDBottleneckBlock(ResNetBottleneckBlock):\n",
    "    skip_cls: ModuleDef = ResNetDSkipConnection\n",
    "\n",
    "class ResNeStBottleneckBlock(ResNetBottleneckBlock):\n",
    "    skip_cls: ModuleDef = ResNeStSkipConnection\n",
    "    avg_pool_first: bool = False\n",
    "    radix: int = 2\n",
    "\n",
    "    splat_cls: ModuleDef = SplAtConv2d\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        assert self.radix == 2  # TODO: implement radix != 2\n",
    "\n",
    "        skip_cls = partial(self.skip_cls, conv_block_cls=self.conv_block_cls)\n",
    "        group_width = int(self.n_hidden * (self.base_width / 64.)) * self.groups\n",
    "\n",
    "        y = self.conv_block_cls(group_width, kernel_size=(1, 1))(x)\n",
    "\n",
    "        if self.strides != (1, 1) and self.avg_pool_first:\n",
    "            y = nn.avg_pool(y, (3, 3), strides=self.strides, padding=[(1, 1), (1, 1)])\n",
    "\n",
    "        y = self.splat_cls(group_width,\n",
    "                           kernel_size=(3, 3),\n",
    "                           strides=(1, 1),\n",
    "                           padding=[(1, 1), (1, 1)],\n",
    "                           groups=self.groups,\n",
    "                           radix=self.radix)(y)\n",
    "\n",
    "        if self.strides != (1, 1) and not self.avg_pool_first:\n",
    "            y = nn.avg_pool(y, (3, 3), strides=self.strides, padding=[(1, 1), (1, 1)])\n",
    "\n",
    "        y = self.conv_block_cls(self.n_hidden * self.expansion,\n",
    "                                kernel_size=(1, 1),\n",
    "                                is_last=True)(y)\n",
    "\n",
    "        return self.activation(y + skip_cls(self.strides)(x, y.shape))\n",
    "\n",
    "def ResNet(\n",
    "    block_cls: ModuleDef,\n",
    "    *,\n",
    "    stage_sizes: Sequence[int],\n",
    "    n_classes: int,\n",
    "    hidden_sizes: Sequence[int] = (64, 128, 256, 512),\n",
    "    conv_cls: ModuleDef = nn.Conv,\n",
    "    norm_cls: Optional[ModuleDef] = partial(nn.BatchNorm, momentum=0.9),\n",
    "    conv_block_cls: ModuleDef = ConvBlock,\n",
    "    stem_cls: ModuleDef = ResNetStem,\n",
    "    pool_fn: Callable = partial(nn.max_pool,\n",
    "                                window_shape=(3, 3),\n",
    "                                strides=(2, 2),\n",
    "                                padding=((1, 1), (1, 1))),\n",
    ") -> Sequential:\n",
    "    conv_block_cls = partial(conv_block_cls, conv_cls=conv_cls, norm_cls=norm_cls)\n",
    "    stem_cls = partial(stem_cls, conv_block_cls=conv_block_cls)\n",
    "    block_cls = partial(block_cls, conv_block_cls=conv_block_cls)\n",
    "\n",
    "    layers = [stem_cls(), pool_fn]\n",
    "\n",
    "    for i, (hsize, n_blocks) in enumerate(zip(hidden_sizes, stage_sizes)):\n",
    "        for b in range(n_blocks):\n",
    "            strides = (1, 1) if i == 0 or b != 0 else (2, 2)\n",
    "            layers.append(block_cls(n_hidden=hsize, strides=strides))\n",
    "\n",
    "    layers.append(partial(jnp.mean, axis=(1, 2)))  # global average pool\n",
    "    layers.append(nn.Dense(n_classes))\n",
    "    return Sequential(layers)\n",
    "\n",
    "#RESNET 101 model\n",
    "ResNet101 = partial(ResNet, stage_sizes=STAGE_SIZES[101],stem_cls=ResNetStem, block_cls=ResNetBottleneckBlock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PyTorchTensor = Any\n",
    "def pretrained_resnet(\n",
    "    size: int,\n",
    "    state_dict: Optional[Mapping[str, PyTorchTensor]] = None\n",
    ") -> Tuple[ModuleDef, FrozenDict]:\n",
    "    \"\"\"Returns pretrained variables for ResNet ported from torch.hub.\n",
    "    \"\"\"\n",
    "    PATH =  \"/notebooks/storage/resnet101-63fe2227.pth\"  \n",
    "    state_dict= torch.load(PATH)\n",
    "    pt2jax: Dict[str, Sequence[str]] = {}\n",
    "    add_bn = _get_add_bn(pt2jax)\n",
    "    pt2jax['conv1.weight'] = ('params', 'layers_0', 'ConvBlock_0', 'Conv_0', 'kernel')\n",
    "    add_bn('bn1', ('layers_0', 'ConvBlock_0', 'BatchNorm_0'))\n",
    "\n",
    "    lyr = 2  # block_ind\n",
    "    for b, n_blocks in enumerate (STAGE_SIZES[101], 1):\n",
    "        for i in range(n_blocks):\n",
    "            for j in range(2 + (size >= 50)):\n",
    "                pt2jax[f'layer{b}.{i}.conv{j+1}.weight'] = ('params', f'layers_{lyr}',\n",
    "                                                            f'ConvBlock_{j}', 'Conv_0',\n",
    "                                                            'kernel')\n",
    "                add_bn(f'layer{b}.{i}.bn{j+1}',\n",
    "                       (f'layers_{lyr}', f'ConvBlock_{j}', 'BatchNorm_0'))\n",
    "\n",
    "            if f'layer{b}.{i}.downsample.0.weight' in state_dict:\n",
    "                pt2jax[f'layer{b}.{i}.downsample.0.weight'] = ('params',\n",
    "                                                               f'layers_{lyr}',\n",
    "                                                               'ResNetSkipConnection_0',\n",
    "                                                               'ConvBlock_0', 'Conv_0',\n",
    "                                                               'kernel')\n",
    "                add_bn(f'layer{b}.{i}.downsample.1',\n",
    "                       (f'layers_{lyr}', 'ResNetSkipConnection_0', 'ConvBlock_0',\n",
    "                        'BatchNorm_0'))\n",
    "\n",
    "            lyr += 1\n",
    "\n",
    "    lyr += 1\n",
    "    pt2jax['fc.weight'] = ('params', f'layers_{lyr}', 'kernel')\n",
    "    pt2jax['fc.bias'] = ('params', f'layers_{lyr}', 'bias')\n",
    "    variables = _pytorch_to_jax_params(pt2jax, state_dict, ('fc.weight',))\n",
    "    model_cls = partial(ResNet101, n_classes=1000)\n",
    "   \n",
    "    return model_cls, freeze(unflatten_dict(variables))\n",
    "\n",
    "def _pytorch_to_jax_params(pt2jax, state_dict, fc_keys):\n",
    "    variables = {}\n",
    "    for pt_name, jax_key in pt2jax.items():\n",
    "        w = state_dict[pt_name].detach().numpy()\n",
    "        if w.ndim == 4:\n",
    "            w = w.transpose((2, 3, 1, 0))\n",
    "        elif pt_name in fc_keys:\n",
    "            w = w.transpose()\n",
    "        variables[jax_key] = w\n",
    "\n",
    "    return variables\n",
    "\n",
    "def _get_add_bn(pt2jax):\n",
    "    def add_bn(pname, jprefix):\n",
    "        pt2jax[f'{pname}.weight'] = ('params', *jprefix, 'scale')\n",
    "        pt2jax[f'{pname}.bias'] = ('params', *jprefix, 'bias')\n",
    "        pt2jax[f'{pname}.running_mean'] = ('batch_stats', *jprefix, 'mean')\n",
    "        pt2jax[f'{pname}.running_var'] = ('batch_stats', *jprefix, 'var')\n",
    "\n",
    "    return add_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load resnet100 model\n",
    "RESNET100, variables = pretrained_resnet(101)\n",
    "model = RESNET100()\n",
    "model_out=model.apply(variables, jnp.ones((1, 224, 224, 3)) ,mutable=False)\n",
    "print(np.shape(model_out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Image Classification through resnet101-------------\n",
    "\n",
    "#image transforms\n",
    "import torchvision.transforms as T\n",
    "img_res= Image.open('/notebooks/storage/data/bird.jpg')\n",
    "plt.imshow(img_res)\n",
    "preprocess = T.Compose([T.Resize((224,224)),\n",
    "            T.ToTensor(),T.Normalize(mean = [0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])])\n",
    "inp_img_tensor = preprocess(img_res)\n",
    "inp_batch= torch.unsqueeze(inp_img_tensor,0).view(1,224,224,3)\n",
    "out=model.apply(variables, inp_batch, mutable= False)\n",
    "\n",
    "#-------classify-----------\n",
    "\n",
    "with open('/notebooks/storage/imagenet_classes.txt') as f: # path to the ImageNet labels\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "max_index=jax.numpy.argmax(out,1) \n",
    "percentage= jax.nn.softmax(out, axis=1) *100\n",
    "print(\"The image is classified as (\",classes[max_index[0]],\") with percentage \", percentage[0][max_index[0]].item() ,\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-moore",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
