{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from typing import Any, Dict, Mapping, Optional, Sequence, Tuple, Callable ,Union, Iterable\n",
    "\n",
    "import flax\n",
    "from flax.core import FrozenDict, freeze\n",
    "from flax.traverse_util import unflatten_dict\n",
    "from flax import linen as nn\n",
    "from flax.linen import BatchNorm, Conv\n",
    "from flax.core.frozen_dict import freeze, unfreeze\n",
    "PyTorchTensor = Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-sister",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install torchvision==0.11.1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-tuesday",
   "metadata": {},
   "source": [
    "### Resnet model structure in JAX ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet model structure in jax\n",
    "STAGE_SIZES = {\n",
    "    18: [2, 2, 2, 2], 34: [3, 4, 6, 3], 50: [3, 4, 6, 3],\n",
    "    101: [3, 4, 23, 3], 152: [3, 8, 36, 3], 200: [3, 24, 36, 3],\n",
    "    269: [3, 30, 48, 8],\n",
    "}\n",
    "ModuleDef = Callable[..., Callable]\n",
    "InitFn = Callable[[Any, Iterable[int], Any], Any]\n",
    "#conv \n",
    "class ConvBlock(nn.Module): \n",
    "        \n",
    "        n_filters: int\n",
    "        dilation: int  \n",
    "        kernel_size: Tuple[int, int] = (3, 3)\n",
    "        strides: Tuple[int, int] = (1, 1)\n",
    "        \n",
    "        activation: Callable = nn.relu\n",
    "        padding: Union[str, Iterable[Tuple[int, int]]] = ((0, 0), (0, 0))\n",
    "        is_last: bool = False\n",
    "        groups: int = 1\n",
    "        kernel_init: InitFn = nn.initializers.kaiming_normal()\n",
    "        bias_init: InitFn = nn.initializers.zeros\n",
    "        conv_cls: ModuleDef = nn.Conv\n",
    "        norm_cls: Optional[ModuleDef] = partial(nn.BatchNorm, momentum=0.9)\n",
    "        force_conv_bias: bool = False\n",
    "\n",
    "        @nn.compact\n",
    "        def __call__(self, x):\n",
    "            x = self.conv_cls(\n",
    "                self.n_filters,\n",
    "                self.kernel_size,\n",
    "                self.strides,              \n",
    "                use_bias=(not self.norm_cls or self.force_conv_bias),\n",
    "                padding=self.padding,\n",
    "                feature_group_count=self.groups,\n",
    "                kernel_init=self.kernel_init,\n",
    "                bias_init=self.bias_init,\n",
    "                \n",
    "                kernel_dilation=(self.dilation,self.dilation) \n",
    "            )(x)\n",
    "            if self.norm_cls:\n",
    "                scale_init = (nn.initializers.zeros\n",
    "                              if self.is_last else nn.initializers.ones)\n",
    "                mutable = self.is_mutable_collection('batch_stats')\n",
    "                x = self.norm_cls(use_running_average=not mutable, scale_init=scale_init)(x)\n",
    "\n",
    "            if not self.is_last:\n",
    "                x = self.activation(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "class Sequential(nn.Module):\n",
    "    layers: Sequence[Union[nn.Module, Callable[[jnp.ndarray], jnp.ndarray]]]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "def slice_variables(variables: Mapping[str, Any],\n",
    "                    start: int = 0,\n",
    "                    end: Optional[int] = None) -> flax.core.FrozenDict:\n",
    "    \"\"\"Returns variables dict correspond to a sliced model.\n",
    "    \"\"\"\n",
    "    last_ind = max(int(s.split('_')[-1]) for s in variables['params'])\n",
    "    if end is None:\n",
    "        end = last_ind + 1\n",
    "    elif end < 0:\n",
    "        end += last_ind + 1\n",
    "\n",
    "    sliced_variables: Dict[str, Any] = {}\n",
    "    for k, var_dict in variables.items():  # usually params and batch_stats\n",
    "        sliced_variables[k] = {\n",
    "            f'layers_{i-start}': var_dict[f'layers_{i}']\n",
    "            for i in range(start, end)\n",
    "            if f'layers_{i}' in var_dict\n",
    "        }\n",
    "\n",
    "    return flax.core.freeze(sliced_variables)                \n",
    "\n",
    "#softmax func\n",
    "def rsoftmax(x, radix, cardinality):\n",
    "    # (batch_size, features) -> (batch_size, features)\n",
    "    batch = x.shape[0]\n",
    "    if radix > 1:\n",
    "        x = x.reshape((batch, cardinality, radix, -1)).swapaxes(1, 2)\n",
    "        return nn.softmax(x, axis=1).reshape((batch, -1))\n",
    "    else:\n",
    "        return nn.sigmoid(x)\n",
    "\n",
    "class SplAtConv2d(nn.Module):\n",
    "    channels: int\n",
    "    kernel_size: Tuple[int, int]\n",
    "    strides: Tuple[int, int] = (1, 1)\n",
    "    padding: Union[str, Iterable[Tuple[int, int]]] = ((0, 0), (0, 0))\n",
    "    groups: int = 1\n",
    "    radix: int = 2\n",
    "    reduction_factor: int = 4\n",
    "    conv_block_cls: ModuleDef = ConvBlock\n",
    "    cardinality: int = groups\n",
    "    match_reference: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        inter_channels = max(x.shape[-1] * self.radix // self.reduction_factor, 32)\n",
    "\n",
    "        conv_block = self.conv_block_cls(self.channels * self.radix,\n",
    "                                         kernel_size=self.kernel_size,\n",
    "                                         strides=self.strides,\n",
    "                                         groups=self.groups * self.radix,\n",
    "                                         padding=self.padding)\n",
    "        conv_cls = conv_block.conv_cls  # type: ignore\n",
    "        x = conv_block(x)\n",
    "\n",
    "        if self.radix > 1:\n",
    "            # torch split takes split_size: int(rchannel//self.radix)\n",
    "            # jnp split takes num sections: self.radix\n",
    "            split = jnp.split(x, self.radix, axis=-1)\n",
    "            gap = sum(split)\n",
    "        else:\n",
    "            gap = x\n",
    "\n",
    "        gap = gap.mean((1, 2), keepdims=True)  # type: ignore # global average pool\n",
    "        # Remove force_conv_bias after resolving\n",
    "        # github.com/zhanghang1989/ResNeSt/issues/125\n",
    "        gap = self.conv_block_cls(inter_channels,\n",
    "                                  kernel_size=(1, 1),\n",
    "                                  groups=self.cardinality,\n",
    "                                  force_conv_bias=self.match_reference)(gap)\n",
    "\n",
    "        attn = conv_cls(self.channels * self.radix,\n",
    "                        kernel_size=(1, 1),\n",
    "                        feature_group_count=self.cardinality)(gap)  # n x 1 x 1 x c\n",
    "        attn = attn.reshape((x.shape[0], -1))\n",
    "        attn = rsoftmax(attn, self.radix, self.cardinality)\n",
    "        attn = attn.reshape((x.shape[0], 1, 1, -1))\n",
    "\n",
    "        if self.radix > 1:\n",
    "            attns = jnp.split(attn, self.radix, axis=-1)\n",
    "            out = sum(a * s for a, s in zip(attns, split))\n",
    "        else:\n",
    "            out = attn * x\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNetStem(nn.Module):\n",
    "    conv_block_cls: ModuleDef = ConvBlock\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        return self.conv_block_cls(64,dilation=1,\n",
    "                                   kernel_size=(7, 7),\n",
    "                                   strides=(2, 2),\n",
    "                                   padding=[(3, 3), (3, 3)])(x) \n",
    "\n",
    "class ResNetSkipConnection(nn.Module):\n",
    "    strides: Tuple[int, int]\n",
    "    conv_block_cls: ModuleDef = ConvBlock\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, out_shape):\n",
    "        if x.shape != out_shape:\n",
    "            x = self.conv_block_cls(out_shape[-1],dilation=1,\n",
    "                                    kernel_size=(1, 1),\n",
    "                                    strides=self.strides,\n",
    "                                    activation=lambda y: y)(x)\n",
    "        return x\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    n_hidden: int\n",
    "    strides: Tuple[int, int] = (1, 1)\n",
    "    activation: Callable = nn.relu\n",
    "    conv_block_cls: ModuleDef = ConvBlock\n",
    "    skip_cls: ModuleDef = ResNetSkipConnection\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        skip_cls = partial(self.skip_cls, conv_block_cls=self.conv_block_cls)\n",
    "        y = self.conv_block_cls(self.n_hidden,\n",
    "                                padding=[(1, 1), (1, 1)],\n",
    "                                strides=self.strides)(x)\n",
    "        y = self.conv_block_cls(self.n_hidden, padding=[(1, 1), (1, 1)],\n",
    "                                is_last=True)(y)\n",
    "        return self.activation(y + skip_cls(self.strides)(x, y.shape))\n",
    "\n",
    "class ResNetBottleneckBlock(nn.Module):\n",
    "    \n",
    "    dilation: int \n",
    "    n_hidden: int\n",
    "    strides: Tuple[int, int] ##\n",
    "    expansion: int = 4\n",
    "    groups: int = 1  # cardinality\n",
    "    base_width: int = 64\n",
    "    activation: Callable = nn.relu\n",
    "    conv_block_cls: ModuleDef = ConvBlock\n",
    "    skip_cls: ModuleDef = ResNetSkipConnection\n",
    "   \n",
    "    @nn.compact\n",
    "    def __call__(self, x):   \n",
    "        skip_cls = partial(self.skip_cls, conv_block_cls=self.conv_block_cls)\n",
    "        group_width = int(self.n_hidden * (self.base_width / 64.)) * self.groups\n",
    "        # Downsampling strides in 3x3 conv instead of 1x1 conv, which improves accuracy.    \n",
    "        y = self.conv_block_cls(group_width,dilation=1, kernel_size=(1, 1))(x)   \n",
    "        y = self.conv_block_cls(group_width, strides=self.strides,\n",
    "                                groups =self.groups,\n",
    "                                padding =((self.dilation, self.dilation), (self.dilation, self.dilation)) ,\n",
    "                                dilation=self.dilation)(y) #as padding=dilation in conv3x3\n",
    "       \n",
    "        y = self.conv_block_cls(self.n_hidden * self.expansion, dilation=1,\n",
    "                                kernel_size =(1, 1),\n",
    "                                is_last=True)(y)   \n",
    "        \n",
    "        return self.activation(y + skip_cls(self.strides)(x, y.shape))\n",
    "    \n",
    "#=----------------------------------------------------------------------------------------------------\n",
    "def ResNet(\n",
    "    block_cls: ModuleDef,\n",
    "    *,\n",
    "    stage_sizes: Sequence[int],\n",
    "    n_classes: int,\n",
    "    hidden_sizes: Sequence[int] = (64, 128, 256, 512),\n",
    "    conv_cls: ModuleDef = nn.Conv,\n",
    "    norm_cls: Optional[ModuleDef] = partial(nn.BatchNorm, momentum=0.9),\n",
    "    conv_block_cls: ModuleDef = ConvBlock,\n",
    "    stem_cls: ModuleDef = ResNetStem,\n",
    "    pool_fn: Callable = partial(nn.max_pool,\n",
    "                                window_shape=(3, 3),\n",
    "                                strides=(2, 2),\n",
    "                                padding=((1, 1), (1, 1))),\n",
    ") -> Sequential:\n",
    "    conv_block_cls = partial(conv_block_cls, conv_cls=conv_cls, norm_cls=norm_cls)\n",
    "    stem_cls = partial(stem_cls, conv_block_cls=conv_block_cls)\n",
    "    block_cls = partial(block_cls, conv_block_cls=conv_block_cls)\n",
    "\n",
    "    layers = [stem_cls(), pool_fn]\n",
    "\n",
    "    for i, (hsize, n_blocks) in enumerate(zip(hidden_sizes, stage_sizes)):\n",
    "        for b in range(n_blocks):\n",
    "            strides = (1, 1) if i == 0 or b != 0 else (2, 2)\n",
    "            layers.append(block_cls(n_hidden=hsize, strides=strides))\n",
    "\n",
    "    layers.append(partial(jnp.mean, axis=(1, 2)))  # global average pool\n",
    "    layers.append(nn.Dense(n_classes))\n",
    "    return Sequential(layers)\n",
    "\n",
    "#RESNET 101 model\n",
    "ResNet101 = partial(ResNet, stage_sizes=STAGE_SIZES[101],stem_cls=ResNetStem, block_cls=ResNetBottleneckBlock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet_layer4(\n",
    "    block_cls: ModuleDef,\n",
    "    *,\n",
    "    stage_sizes: Sequence[int],\n",
    "    n_classes: int,\n",
    "    hidden_sizes: Sequence[int] = (64, 128, 256, 512),\n",
    "    conv_cls: ModuleDef = nn.Conv,\n",
    "    norm_cls: Optional[ModuleDef] = partial(nn.BatchNorm, momentum=0.9),\n",
    "    conv_block_cls: ModuleDef = ConvBlock,\n",
    "    stem_cls: ModuleDef = ResNetStem,\n",
    "    pool_fn: Callable = partial(nn.max_pool,\n",
    "                                window_shape=(3, 3),\n",
    "                                strides=(2, 2),\n",
    "                                padding=((1, 1), (1, 1))),\n",
    "    replace_stride_with_dilation: Optional[Sequence[bool]] = None  \n",
    "    \n",
    ") -> Sequential:\n",
    "    \n",
    "    conv_block_cls = partial(conv_block_cls, conv_cls=conv_cls, norm_cls=norm_cls)\n",
    "    stem_cls = partial(stem_cls, conv_block_cls=conv_block_cls)\n",
    "    block_cls = partial(block_cls, conv_block_cls=conv_block_cls)\n",
    "    layers = [stem_cls(), pool_fn]\n",
    "    if replace_stride_with_dilation is None: \n",
    "        # each element in the tuple indicates if we should replace\n",
    "        # the 2x2 stride with a dilated convolution instead\n",
    "        replace_stride_with_dilation = [False, False, False]\n",
    "    if len(replace_stride_with_dilation) != 3:\n",
    "        raise ValueError(\n",
    "            \"replace_stride_with_dilation should be None \"\n",
    "            \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)\n",
    "        )\n",
    "        \n",
    "    dilation=1\n",
    "    previous_dilation=1\n",
    "    for i, (hsize, n_blocks) in enumerate(zip(hidden_sizes, stage_sizes)):\n",
    "        b=0\n",
    "        strides = (1, 1) if i == 0 or b != 0 else (2, 2)\n",
    "        previous_dilation = dilation \n",
    "        if i== 0:\n",
    "            dilate=False\n",
    "        else:\n",
    "            dilate= replace_stride_with_dilation[i-1]\n",
    "        \n",
    "        if dilate:\n",
    "                dilation= dilation* strides[0]\n",
    "                strides= (1,1)\n",
    "                previous_dilation = int(dilation/2) #update prev dilation   \n",
    "       \n",
    "        if i==0:\n",
    "                layers.append(block_cls(dilation=1, n_hidden=hsize, strides=strides))   \n",
    "        else:\n",
    "                layers.append(block_cls(dilation=previous_dilation, n_hidden=hsize, strides=strides ))  \n",
    "        \n",
    "        for b in range(1,n_blocks):\n",
    "            strides = (1, 1) if i == 0 or b != 0 else (2, 2)\n",
    "            if i==0:\n",
    "                layers.append(block_cls(dilation =1, n_hidden=hsize, strides=strides)) \n",
    "            else:                          \n",
    "                layers.append(block_cls(dilation=dilation, n_hidden=hsize, strides=strides ))    \n",
    "\n",
    "    return Sequential(layers)\n",
    "\n",
    "ResNet_layer4 = partial(ResNet_layer4, stage_sizes=STAGE_SIZES[101],stem_cls=ResNetStem,\n",
    "                        block_cls=ResNetBottleneckBlock, replace_stride_with_dilation=[False, True, True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pytorch_to_jax_params(pt2jax, state_dict, fc_keys):\n",
    "    variables = {}\n",
    "   # print(pt2jax.items())\n",
    "    for pt_name, jax_key in pt2jax.items():\n",
    "        w = state_dict[pt_name].detach().numpy()\n",
    "        if w.ndim == 4:\n",
    "            w = w.transpose((2, 3, 1, 0))\n",
    "        elif pt_name in fc_keys:\n",
    "            w = w.transpose()\n",
    "        variables[jax_key] = w \n",
    "    return variables\n",
    "\n",
    "def _get_add_bn(pt2jax):\n",
    "    def add_bn(pname, jprefix):\n",
    "        pt2jax[f'{pname}.weight'] = ('params', *jprefix, 'scale')\n",
    "        pt2jax[f'{pname}.bias'] = ('params', *jprefix, 'bias')\n",
    "        pt2jax[f'{pname}.running_mean'] = ('batch_stats', *jprefix, 'mean')\n",
    "        pt2jax[f'{pname}.running_var'] = ('batch_stats', *jprefix, 'var')\n",
    "    return add_bn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_resnet(\n",
    "    size: int,\n",
    "    state_dict: Optional[Mapping[str, PyTorchTensor]] = None\n",
    ") -> Tuple[ModuleDef, FrozenDict]:\n",
    "    \"\"\"Returns pretrained variables for ResNet ported from torch.hub.\n",
    "    Returns:Module Class and variables dictionary for Flax ResNet.\n",
    "    \"\"\"\n",
    "    PATH =  \"/notebooks/storage/resnet101-63fe2227.pth\"   \n",
    "    state_dict= torch.load(PATH)\n",
    "    pt2jax: Dict[str, Sequence[str]] = {}\n",
    "    add_bn = _get_add_bn(pt2jax)\n",
    "    pt2jax['conv1.weight'] = ('params', 'layers_0', 'ConvBlock_0', 'Conv_0', 'kernel')\n",
    "    add_bn('bn1', ('layers_0', 'ConvBlock_0', 'BatchNorm_0'))\n",
    "    lyr = 2  # block_ind\n",
    "    for b, n_blocks in enumerate (STAGE_SIZES[101], 1):\n",
    "        for i in range(n_blocks):\n",
    "            for j in range(2 + (size >= 50)):\n",
    "                pt2jax[f'layer{b}.{i}.conv{j+1}.weight'] = ('params', f'layers_{lyr}',\n",
    "                                                            f'ConvBlock_{j}', 'Conv_0',\n",
    "                                                            'kernel')\n",
    "                add_bn(f'layer{b}.{i}.bn{j+1}',\n",
    "                       (f'layers_{lyr}', f'ConvBlock_{j}', 'BatchNorm_0'))\n",
    "\n",
    "            if f'layer{b}.{i}.downsample.0.weight' in state_dict:\n",
    "                pt2jax[f'layer{b}.{i}.downsample.0.weight'] = ('params',\n",
    "                                                               f'layers_{lyr}',\n",
    "                                                               'ResNetSkipConnection_0',\n",
    "                                                               'ConvBlock_0', 'Conv_0',\n",
    "                                                               'kernel')\n",
    "                add_bn(f'layer{b}.{i}.downsample.1',(f'layers_{lyr}', 'ResNetSkipConnection_0', \n",
    "                        'ConvBlock_0','BatchNorm_0'))\n",
    "            lyr += 1\n",
    "\n",
    "    lyr += 1\n",
    "    pt2jax['fc.weight'] = ('params', f'layers_{lyr}', 'kernel')\n",
    "    pt2jax['fc.bias'] = ('params', f'layers_{lyr}', 'bias')\n",
    "    variables = _pytorch_to_jax_params(pt2jax, state_dict, ('fc.weight',))\n",
    "    model_cls = partial(ResNet101, n_classes=1000) \n",
    "    \n",
    "    return model_cls, freeze(unflatten_dict(variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_customized_resnet(\n",
    "    \n",
    "    size: int,\n",
    "    state_dict: Optional[Mapping[str, PyTorchTensor]] = None\n",
    ") -> Tuple[ModuleDef, FrozenDict]:\n",
    "\n",
    "    PATH =  \"/notebooks/storage/resnet101-63fe2227.pth\"   \n",
    "    state_dict= torch.load(PATH)  \n",
    "    pt2jax: Dict[str, Sequence[str]] = {}\n",
    "    add_bn = _get_add_bn(pt2jax)\n",
    "    pt2jax['conv1.weight'] = ('params', 'layers_0', 'ConvBlock_0', 'Conv_0', 'kernel')\n",
    "    add_bn('bn1', ('layers_0', 'ConvBlock_0', 'BatchNorm_0'))\n",
    "    lyr = 2  # block_ind  \n",
    "    for b, n_blocks in enumerate (STAGE_SIZES[101], 1):\n",
    "        for i in range(n_blocks):\n",
    "            for j in range(2 + (size >= 50)):\n",
    "                pt2jax[f'layer{b}.{i}.conv{j+1}.weight'] = ('params', f'layers_{lyr}',\n",
    "                                                            f'ConvBlock_{j}', 'Conv_0',\n",
    "                                                            'kernel')\n",
    "                add_bn(f'layer{b}.{i}.bn{j+1}',\n",
    "                       (f'layers_{lyr}', f'ConvBlock_{j}', 'BatchNorm_0'))\n",
    "\n",
    "            if f'layer{b}.{i}.downsample.0.weight' in state_dict:\n",
    "                pt2jax[f'layer{b}.{i}.downsample.0.weight'] = ('params',\n",
    "                                                               f'layers_{lyr}',\n",
    "                                                               'ResNetSkipConnection_0',\n",
    "                                                               'ConvBlock_0', 'Conv_0',\n",
    "                                                               'kernel')\n",
    "                add_bn(f'layer{b}.{i}.downsample.1',\n",
    "                       (f'layers_{lyr}', 'ResNetSkipConnection_0', 'ConvBlock_0',\n",
    "                        'BatchNorm_0'))\n",
    "            lyr += 1\n",
    "\n",
    "    lyr += 1\n",
    "    pt2jax['fc.weight'] = ('params', f'layers_{lyr}', 'kernel')\n",
    "    pt2jax['fc.bias'] = ('params', f'layers_{lyr}', 'bias')\n",
    "\n",
    "    variables = _pytorch_to_jax_params(pt2jax, state_dict, ('fc.weight',))\n",
    "    model_cls = partial(ResNet_layer4, n_classes=1000) # customized model from layer4\n",
    "\n",
    "    return model_cls, freeze(unflatten_dict(variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FCNHead classifier in FLAX  \n",
    "class FCNHead(nn.Module):    \n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        in_channels=2048 #num_classes = 21\n",
    "        channels= 21\n",
    "        inter_channels = in_channels // 4  \n",
    "        x= nn.Conv(inter_channels ,kernel_size = (3,3), padding=[(1,1),(1,1)], use_bias =False)(x) \n",
    "        x= nn.BatchNorm(use_running_average=True, momentum=0.9, epsilon=1e-5, dtype=jnp.float32)(x)\n",
    "        x= nn.relu(x)\n",
    "        x= nn.Dropout(0.1,deterministic= True)(x) #but through (training) set deterministic False\n",
    "        x= nn.Conv(channels,kernel_size= (1,1))(x)\n",
    "         \n",
    "        return x   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCN_classifier(\n",
    "    size: int,\n",
    "    state_dict: Optional[Mapping[str, PyTorchTensor]] = None\n",
    ") -> Tuple[ModuleDef, FrozenDict]:\n",
    "    \n",
    "        PATH= \"/notebooks/storage/fcn_resnet101_coco-7ecb50ca.pth\" #classifier state dictionary \n",
    "        state_dict= torch.load(PATH)\n",
    "        pt2jax: Dict[str, Sequence[str]] = {}\n",
    "        pt2jax['classifier.0.weight'] = ('params', 'Conv_0', 'kernel')\n",
    "\n",
    "        pt2jax['classifier.1.weight'] = ('params',  'BatchNorm_0', 'scale')\n",
    "        pt2jax['classifier.1.bias'] = ('params', 'BatchNorm_0', 'bias')\n",
    "        \n",
    "        pt2jax['classifier.1.running_mean'] = ('batch_stats','BatchNorm_0' , 'mean')\n",
    "        pt2jax['classifier.1.running_var'] = ('batch_stats', 'BatchNorm_0', 'var')\n",
    "        \n",
    "        pt2jax['classifier.4.weight'] = ('params', 'Conv_1', 'kernel')\n",
    "        pt2jax['classifier.4.bias'] = ('params', 'Conv_1', 'bias')\n",
    "\n",
    "        variables = _pytorch_to_jax_params(pt2jax, state_dict, ())\n",
    "        model_cls = FCNHead        # get the model structure from FCNHead\n",
    "        \n",
    "        return model_cls, freeze(unflatten_dict(variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for customized resnet layer4- backbone\n",
    "def backbone_pretrained_customized_resnet(\n",
    "    \n",
    "    size: int,\n",
    "    state_dict: Optional[Mapping[str, PyTorchTensor]] = None\n",
    ") -> Tuple[ModuleDef, FrozenDict]:  \n",
    "    PATH= \"/notebooks/storage/fcn_resnet101_coco-7ecb50ca.pth\" #classifier state dictionary\n",
    "    state_dict= torch.load(PATH)\n",
    "    pt2jax: Dict[str, Sequence[str]] = {}\n",
    "    add_bn = _get_add_bn(pt2jax)\n",
    "    \n",
    "    pt2jax['backbone.conv1.weight'] = ('params', 'layers_0', 'ConvBlock_0', 'Conv_0', 'kernel')\n",
    "    add_bn('backbone.bn1', ('layers_0', 'ConvBlock_0', 'BatchNorm_0'))\n",
    "    \n",
    "    lyr = 2 #10 # block_ind\n",
    "    for b, n_blocks in enumerate (STAGE_SIZES[101], 1):\n",
    "        for i in range(n_blocks):\n",
    "            for j in range(2 + (size >= 50)):\n",
    "                pt2jax[f'backbone.layer{b}.{i}.conv{j+1}.weight'] = ('params', f'layers_{lyr}',\n",
    "                                                            f'ConvBlock_{j}', 'Conv_0',\n",
    "                                                            'kernel')\n",
    "                add_bn(f'backbone.layer{b}.{i}.bn{j+1}',\n",
    "                       (f'layers_{lyr}', f'ConvBlock_{j}', 'BatchNorm_0'))\n",
    "\n",
    "            if f'backbone.layer{b}.{i}.downsample.0.weight' in state_dict:\n",
    "                pt2jax[f'backbone.layer{b}.{i}.downsample.0.weight'] = ('params',\n",
    "                                                               f'layers_{lyr}',\n",
    "                                                               'ResNetSkipConnection_0',\n",
    "                                                               'ConvBlock_0', 'Conv_0',\n",
    "                                                               'kernel')\n",
    "                add_bn(f'backbone.layer{b}.{i}.downsample.1',\n",
    "                       (f'layers_{lyr}', 'ResNetSkipConnection_0', 'ConvBlock_0',\n",
    "                        'BatchNorm_0'))\n",
    "            lyr += 1\n",
    "   \n",
    "    lyr += 1\n",
    "    pt2jax['backbone.bn1.running_mean']= ('batch_stats', f'layers_0' ,'ConvBlock_0','BatchNorm_0', 'mean')\n",
    "    pt2jax['backbone.bn1.running_var'] = ('batch_stats', f'layers_0' , 'ConvBlock_0','BatchNorm_0', 'var')\n",
    "\n",
    "    variables = _pytorch_to_jax_params(pt2jax, state_dict, ())\n",
    "    model_cls = partial(ResNet_layer4, n_classes=1000) \n",
    "    \n",
    "    return model_cls, freeze(unflatten_dict(variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fcn_resnet():\n",
    "    \n",
    "    fcn = FCNHead()   \n",
    "    FCNClass_weights=FCN_classifier(101)[1] # catch the loaded weights (model_cls,wights)\n",
    "  \n",
    "    #backbone_pretrained_customized model layer4\n",
    "    bck_RESNET100, bck_variables = backbone_pretrained_customized_resnet(101)\n",
    "    backbone = bck_RESNET100()\n",
    "    backbone_out=backbone.apply(bck_variables, jnp.ones((1,224, 224,3)) ,mutable=False) #initialization   \n",
    "    init_variables= fcn.init(jax.random.PRNGKey(0),backbone_out) #random initialization \n",
    "    class_variables = unfreeze(init_variables)\n",
    "    class_variables.update(FCNClass_weights)\n",
    "    class_variables = freeze(class_variables)\n",
    "   \n",
    "    return (backbone, bck_variables, fcn ,class_variables)\n",
    "    \n",
    "def fcn_resnet101(pretrained: bool = False):\n",
    "    model = _fcn_resnet()  \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-translation",
   "metadata": {},
   "source": [
    "### Image Transformation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img transforms\n",
    "import torchvision.transforms as T\n",
    "img_res= Image.open('/notebooks/storage/data/bird.jpg')   #.resize((224,224))\n",
    "preprocess = T.Compose([T.Resize((224,224)),\n",
    "            T.ToTensor(),T.Normalize(mean = [0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])])\n",
    "inp_img_tensor = preprocess(img_res)\n",
    "inp_batch= torch.unsqueeze(inp_img_tensor,0).permute(0,2,3,1) # Input nchw(0,1,2,3)--- o/p nhwc(0,2,3,1) \n",
    "print(inp_batch.shape)\n",
    "\n",
    "plt.imshow(img_res); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-refund",
   "metadata": {},
   "source": [
    "### Running the Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-designation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Running model\n",
    "fcn_model = fcn_resnet101(pretrained=True)\n",
    "backbone= fcn_model[0]\n",
    "backbone_variables= fcn_model[1]\n",
    "classifier= fcn_model[2]\n",
    "classifier_variables= fcn_model[3]\n",
    "\n",
    "Backbone_out=backbone.apply(backbone_variables, inp_batch, mutable= False) #inp_batch ([1, 3, 224, 224])\n",
    "print(\"Bckbone_out:\", jax.tree_map(lambda x: x.shape, Backbone_out))\n",
    "#print(\"Bck_out layer:\", jax.tree_map(lambda x: x, Backbone_out))\n",
    "\n",
    "classifier_out=classifier.apply(classifier_variables, Backbone_out , mutable= False) #Backbone_out: (1, 28, 28, 2048)\n",
    "print(\"Classifier output:\",classifier_out.shape) #(1, 28, 28, 21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the helper function\n",
    "def decode_segmap(image, nc=21):\n",
    "    \n",
    "    label_colors = np.array([(0, 0, 0),  # 0=background\n",
    "                    # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n",
    "                    (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),\n",
    "                    # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n",
    "                    (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),\n",
    "                    # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person\n",
    "                    (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128),\n",
    "                    # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n",
    "                    (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)])\n",
    "    \n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "    for l in range(0, nc):\n",
    "        idx = image == l\n",
    "        r[idx] = label_colors[l, 0]\n",
    "        g[idx] = label_colors[l, 1]\n",
    "        b[idx] = label_colors[l, 2]\n",
    "\n",
    "    rgb = np.stack([r, g, b], axis=2)\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-corps",
   "metadata": {},
   "source": [
    "### Segmentation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-fight",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Segmentation\n",
    "om= jax.image.resize(classifier_out.squeeze(), (224,224,21),'bilinear')\n",
    "#print(om.shape)\n",
    "om2= jax.numpy.argmax(om, axis=2, out=None)\n",
    "#print(om2.shape)\n",
    "rgb = decode_segmap(om2)\n",
    "plt.imshow(rgb); plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-vertex",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
